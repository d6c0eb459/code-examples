"""
TensorFlow 1 example.

This is an excerpt from the source code for my master's thesis, 'Visual blending in
sketch drawings using variational autoencoders'. Thesis text (in Spanish) is available
online through the library of the National Autonomous University of Mexico at:
http://tesis.unam.mx.

We are interested in evaluating sketch drawings to see how much they visually represent
each of various categories of drawings like house, washing machine, duck, etc.

Our sketches come from a pre-trained variational autoencoder called sketch-rnn (Ha and
Eck, 2017), which generates novel drawings by iteratively sampling from Gaussian mixture
models generated by an RNN LSTM decoder, itself conditioned on a latent vector.

Thus, more specifically we are interested in determining what parts of the latent space
correspond to drawings that represent each of the aforementioned categories. In other
words, we want to take a latent vector as an input and output a multi-label
classification where the label is high if the drawing visually represents the respective
label. Note that we're specifically interested in drawings with multiple labels, that is,
drawings that visually represent multiple categories of drawings at the same time.

The process of sampling from sketch-rnn's mixture models is slow and non-differentiable,
which makes it imposible to use algorithms like gradient descent. Therefore, we train a
model to approximate the performance of an RNN classifier trained to perform the above
task. The model amortizes the high cost of performing this training by making the process
of converting a latent vector to a multi-label vetor much faster, which in turn enables
rapid discovery of latent spaces which contain many blended drawings.

Copyright 2018 Derek Cheung. All Rights Reserved.

References:
1. David Ha, Douglas Eck. A Neural Representation of Sketch Drawings.
https://arxiv.org/abs/1704.03477
"""

import os
import sys
import time
import tensorflow as tf
import numpy as np
import model as sketch_rnn
import sketch_rnn_train
import rnn

# These are references to other files in this project that are not part of this sample
from trainer import Trainer
import utils
import tbatcher  # threaded batcher
import classifier


def leaky_relu(x, alpha=0.2):
    return tf.nn.relu(x) - alpha * tf.nn.relu(-x)


def get_default_hparams():
    hparams = tf.contrib.training.HParams(
        # configuration
        is_training=True,
        # directory setups
        save_dir="logs",
        srm_name="house-washing-machine",
        cls_name="alpha-classifier",
        test_name="alpha-amortizer",
        # autofill
        data_set=["auto"],
        labels=["auto"],
        label_width="auto",
        comparison_classes=["auto"],
        # optimization parameters
        batch_size=100,
        log_every_secs=10,
        save_every_secs=120,
        learning_rate=1e-3,
        min_learning_rate=1e-5,
        decay_rate=0.99995,
        num_steps=50000,
        critic_size=512,
        critic_layers=5,
    )
    return hparams


def get_srm_hparams(hps):
    hps_srm = sketch_rnn.get_default_hparams()
    path = os.path.join(hps.save_dir, hps.srm_name, "model_config.json")
    with tf.gfile.Open(path, "r") as input_stream:
        hps_srm.parse_json(input_stream.read())
    hps_srm.is_training = False
    hps_srm.use_input_dropout = 0
    hps_srm.use_recurrent_dropout = 0
    hps_srm.use_output_dropout = 0
    hps_srm.is_training = 0
    return hps_srm


def get_cls_hparams(hps):
    hps_cls = classifier.get_default_hparams()
    path = os.path.join(hps.save_dir, hps.cls_name, "model_config.json")
    with tf.gfile.Open(path, "r") as input_stream:
        hps_srm.parse_json(input_stream.read())
    hps_cls.is_training = False
    return hps_cls


class Amortizer(Trainer):
    """
    The amortizer model, which tries to approximate the behaviour of a model that takes
    as its input a latent vector and produces a multi-label classification of the
    sketch drawing that corresponds with the latent vector.
    """

    hps = None
    srm = None  # sketch-rnn model
    srm_s = None  # sketch-rnn sampling model
    clsm = None  # classifier model
    dataset = None
    loss_names = ["loss", "loss_l2", "lr"]

    def get_save_dir(self):
        return os.path.join(self.hps.save_dir, self.hps.test_name)

    def __init__(self, hps, hps_srm=None, hps_cls=None, input_k=None):
        """
        Build the model.
        """
        self.hps = hps

        if "auto" in hps.comparison_classes:
            hps.comparison_classes = hps_srm.data_set

        if hps_srm is not None:
            hps_srms = sketch_rnn.copy_hparams(hps_srm)
            if not self.hps.is_training:
                hps_srms.max_seq_len = 1  # slow sampling
            self.srm_s = sketch_rnn.Model(hps_srms, fast_mode=self.hps.is_training)

        if hps_cls is not None:
            # load in classifier
            self.clsm = classifier.Classifier(hps_cls)

            self.hps.label_width = self.clsm.hps.label_width
            self.hps.labels = self.clsm.hps.labels
            self.hps.data_set = self.clsm.hps.data_set

        self.label_width = self.hps.label_width
        assert isinstance(self.label_width, int)

        with tf.variable_scope("amortizer"):
            self.global_step = tf.get_variable(
                "global_step", dtype=tf.int32, initializer=0, trainable=False
            )

            ######
            # Inputs
            ###

            if input_k is None:
                self.input_k = tf.placeholder_with_default(
                    input=tf.random_uniform(
                        [self.hps.batch_size, hps_srm.z_size], minval=-3, maxval=3
                    ),
                    name="input_k",
                    shape=[self.hps.batch_size, hps_srm.z_size],
                )
            else:
                self.input_k = input_k

            ######
            # Amortizer, fully connected layers
            ###
            hl = self.input_k
            for _ in range(self.hps.critic_layers):
                hl = leaky_relu(tf.layers.dense(hl, self.hps.critic_size))
            self.logits = tf.layers.dense(hl, self.label_width)

            ######
            # Losses
            ###
            if self.hps.is_training:
                self.input_logits = tf.placeholder_with_default(
                    input=self.clsm.logits_c,
                    name="input_logits",
                    shape=[self.hps.batch_size, self.label_width],
                )

                # cross entropy loss
                labels = tf.nn.sigmoid(self.input_logits)
                entropy = tf.nn.sigmoid_cross_entropy_with_logits(
                    labels=labels, logits=self.logits
                )
                self.loss = tf.reduce_mean(entropy)

                # dynamic learning rate
                self.lr = (hps.learning_rate - hps.min_learning_rate) * (
                    hps.decay_rate
                ) ** tf.cast(self.global_step, tf.float32) + hps.min_learning_rate

                # l2 regularization
                keys = tf.get_collection(
                    tf.GraphKeys.TRAINABLE_VARIABLES, scope="amortizer"
                )
                self.loss_l2 = tf.add_n([tf.nn.l2_loss(t) for t in keys]) * 0.0001

                # final loss function
                loss = self.loss + self.loss_l2

                # underlying code uses Adam optimizer
                self.train_op = self.build_optimizer(
                    loss, keys, self.lr, "train_op", global_step=self.global_step
                )

    def restore_all(self, sess, best=False):
        """
        Restores a pre-trained model
        """
        # restore self
        saver = self.restore_self(sess, best, "amortizer")

        # restore classifier to me amortized
        if self.clsm is not None:
            keys = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope="classifier")
            saver_cls = tf.train.Saver(keys)
            (save_path, save_path_best) = self.clsm.get_save_paths()
            saver_cls.restore(sess, save_path)
            global_step = sess.run(self.clsm.global_step)
            assert global_step != 0
            print("Restored classifier at global_step=%i" % global_step)

        # restore sketch-rnn
        if self.srm_s is not None:
            keys = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope="vector_rnn")
            saver_srm = tf.train.Saver(keys)
            try:
                saver_srm.restore(
                    sess,
                    os.path.join(self.hps.save_dir, self.hps.srm_name, "vector_best"),
                )
            except tf.errors.NotFoundError:
                saver_srm.restore(
                    sess, os.path.join(self.hps.save_dir, self.hps.srm_name, "vector")
                )
            global_step = sess.run(self.srm_s.global_step)
            assert global_step != 0
            print("Restored sketch-rnn model at global_step=%i" % global_step)

        return saver

    def sample_random(self, sess):
        """
        Grab a batch of random latent vectors from the latent space.
        """
        # vectors
        z = np.random.randn(self.hps.batch_size, self.srm_s.hps.z_size)

        # convert vectors to sequences of pen strokes using sketch-rnn

        # xs is an array [batch_size, sequence_length, 5] as defined in sketch-rnn
        # ss is an array [batch_size] of sequence lengths as defined in sketch-rnn
        xs, ss = sess.run(
            [self.srm_s.output_x, self.srm_s.output_s], {self.srm_s.batch_z: z}
        )

        # pad / trim the sequences
        xp = np.zeros((self.hps.batch_size, self.clsm.hps.max_seq_len + 1, 5))
        xp[:, :, 4] = 1
        assert self.srm_s.max_seq_len < self.clsm.hps.max_seq_len + 1
        # msl = Maximum Stroke Length
        msl_srm = self.srm_s.hps.max_seq_len
        xp[:, :msl_srm:, :] = xs

        # returns latent vectors, sequences, and sequence lengths
        return (z, xp, ss)

    def sample_step(self, sess):
        # at this point we have a pre-trained model loaded and can run
        # any experiments we want here.
        pass

    def train_step(self, sess, bw, step):
        (z, xs, ss) = self.sample_random(sess)
        (loss, step, _) = sess.run(
            [self.loss, self.global_step, self.train_op],
            {self.input_k: z, self.clsm.input_x: xs, self.clsm.input_s: ss},
        )
        return ([loss], step)


if __name__ == "__main__":
    raise RuntimeError(
        (
            "This is just a code excerpt and will not run without supporting"
            "files from the rest of the project."
        )
    )
    hps = get_default_hparams()
    hps_srm = get_srm_hparams(hps)
    hps_cls = get_cls_hparams(hps)

    model = Amortizer(hps, hps_srm, hps_cls)
    if hps.is_training:
        model.train(save=True)
    else:
        model.sample(best=False)
